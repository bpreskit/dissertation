In this section, we discuss a number of algorithms by which we can recover an estimate of $x_0$ from $T_{\delta, s}(\mathcal{A}^{-1}(y))$.  We begin with \cref{sec:blocky_block}, which discusses some improvements that can be made to the magnitude estimation step of \cref{alg:phaseRetrieval1} (the $\sqrt{X_{j,j}}$ of line 4).  These improvements, which we call ``blockwise'' vector or magnitude estimation, were first implemented in the numerical study of \cref{sec:MagEstImpNumerical}, and while they empirically delivered better results, no proof was yet discovered to guarantee by how much they improve the estimate of magnitudes.  \Cref{sec:pty_std_rec} describes and proves the robustness bounds for an algorithm almost completely analogous to \cref{alg:phaseRetrieval1}, taking advantage of the results in \cref{sec:blocky_block} for the magnitude estimation.  Finally, in \cref{sec:vecky_vec}, we use blockwise vector estimation to devise a completely new recovery algorithm that is quite general with respect to the sets of shifts it permits.  Although its recovery guarantees are not attractive, we will find in \cref{sec:ptych_num} that these theoretical results belie strong empirical performance.

\subsection{Blockwise Magnitude and Vector Estimation}
\label{sec:blocky_block}
For the moment, we restrict our discussion to the special case of dense shifts in our measurements, where $s = 1$ as in the work of \cref{ch:base_model,ch:meas}.  In \cref{sec:MagEstImpNumerical}, it was noted that the technique used to calculate the magnitudes of the entries of $\ux$ from $X \approx T_\delta(\ux \ux^*)$ was functional, but rudimentary.  Recall from line 4 of \cref{alg:phaseRetrieval1} that we have $X = \Ac^{-1}(\Ac(x_0 x_0^*) + n)$, where $\ux \in \Cd$ is the ground truth objective vector, $\Ac : \Cdxd \to \R^{d D}$ is the linear measurement operator determined by the masks $\{m_j\}_{j \in [D]}$, as defined, for example, in \eqref{eq:meas_op} of \cref{sec:meas_intro}, and $n \in \R^{d D}$ is arbitrary noise.  Then $X = \uX + \Ac^{-1}(n)$, where $\uX = T_\delta(\ux \ux^*)$, and we estimate the magnitude of $\ux_i$ by simply taking $\abs{x_i} = \sqrt{X_{ii}} \approx \sqrt{\uX_{ii}} = \abs{\ux_i}$.  This technique works, as we are able to show in \cref{lem:diag_mag_diff}, but empirically, we found that a slightly more sophisticated technique does a much better job here.  While this comparison was not made explicit in \cref{sec:NumEval}, we briefly illustrate it in \cref{sec:ptych_num}.

We notice that taking $\abs{x_i} = X_{ii}$ is equivalent to taking $\abs{x_i}$ to be the rank-1 approximation of the $1 \times 1, i\th$ diagonal block matrix of $X$, namely $\begin{bmatrix} X_{ii} \end{bmatrix}$, since these diagonal blocks are equal to the diagonal blocks of the \emph{untruncated} $\ux \ux^*$ when there is no noise.  However, given the width of the diagonal band in $T_\delta(\Cdxd)$, we could just as easily take blocks of size up to $\delta \times \delta$ and calculate their top eigenvectors; this would give us $2 \delta - 1$ estimates for each entry's magnitude, so we can combine them by averaging them together.  %To denote these blocks, we will set, for $\{j_1, \ldots, j_k\} = J \subset [d], P_{J} =\rowmatfl{e_{j_1}}{e_{j_k}}$, and take $X^{(\ell)} = P_{[\delta]_\ell} X P_{[\delta]_\ell}^*$ so that $X^{(\ell)} \in \C^{\delta \times \delta}$ and $X^{(\ell)}_{ij} = X_{i + \ell - 1, j + \ell - 1}$.  We may then take $\tilde{u}^{(\ell)}$ to be the top eigenvector of $X^{(\ell)}$, normalized such that $\norm{\tilde{u}^{(\ell)}}_2 = \norm{X^{(\ell)}}_2$.  We then use $u^{(\ell)} = S_d^{\ell - 1} \Rc_d(\tilde{u}^{(\ell)}) \in \Cd$ as the ``spatially
To denote these blocks, we will set $\abs{X}^{(\ell)} = \diag(\one_{[\delta]_\ell}) \abs{X} \diag(\one_{[\delta]_\ell})$\footnote{Here, and in the remainder of this section, we emphasize that all indices of objects in $\Cd$ and $\Cdxd$ are taken modulo $d$.} and $u^{(\ell)}$ to be the top eigenvector of $\abs{X}^{(\ell)}$, normalized such that $\norm{u^{(\ell)}}_2 = \norm{\abs{X}^{(\ell)}}_2$.  We then produce our estimate of the magnitudes by taking $\abs{x} = \frac{1}{\delta} \sum_{\ell = 1}^d u^{(\ell)}$.  

Before formally stating this algorithm, we observe a few ways in which it may be generalized.  Firstly, we notice that this method can easily handle arbitrary block sizes $m$ for the blockwise, eigenvector-based magnitude estimations by simply taking $\abs{X}^{(\ell, m)} = \diag(\one_{[m]_\ell}) \abs{X} \diag(\one_{[m]_\ell})$, as long as $m \le \delta$ -- although this would require us to change the denominator in the averaging step, using $\frac{1}{m} \sum_{\ell = 1}^d u^{(\ell, m)}$.  Proceeding even further, we can generalize this technique to use any collection $\{J_i\}_{i = 1}^N, J_i \subset [d]$ satisfying \begin{equation} \begin{gathered} [d] \subset \bigcup_{i = 1}^N J_i \\ \one_{J_i} \one_{J_i}^* \in T_{\delta}(\Cdxd) \end{gathered} \label{eq:T_delta_cover}\end{equation}  We will call any collection satisfying \eqref{eq:T_delta_cover} a \emph{$(T_\delta, d)$-covering} (or just \emph{covering} when $T_\delta$ and $d$ are clear from context), and the process of estimating magnitudes of $\ux$ from $X$ with respect to a $(T_\delta, d)$-covering is described in \cref{alg:blocky_block}.\footnote{We remark that this definition and the recovery algorithm are very obviously extensible to the use of $T_{\delta, s}$ instead of $T_\delta$.  In fact, this is a \emph{restriction}, if we consider in \eqref{eq:T_delta_cover} that $T_{\delta, s} \subset T_\delta$.  The definition, therefore, of a $(T_{\delta, s}, d)$-covering, is made by analogy to \eqref{eq:T_delta_cover}.}  It is worth remarking that the ``averaging step,'' specified in line 3, is optimal in the least-squares sense.  We set $P_{J_i} = \diag(\one_{J_i})$ to be the orthogonal projections onto the coordinate subspace associated with $J_i$ and consider that the vectors $u^{(J_i)}$ (in line 2) represent estimates of the projections $P_{J_i} \abs{\ux}$.  The least squares solution to $P_{J_i} u = u^{(J_i)}$, or \[\colmatfl{P_{J_1}}{P_{J_N}} u = \colmatfl{u^{(J_1)}}{u^{(J_N)}}\] is obtained by taking the pseudoinverse.  In this case, we have \[\colmatfl{P_{J_1}}{P_{J_N}}^* \colmatfl{P_{J_1}}{P_{J_N}} = \sum_{i = 1}^N P_{J_i}^* P_{J_i} = \sum_{i = 1}^N P_{J_i} = \diag(\mu),\] with $\mu_j = \abs{\{i : j \in J_i\}}$ as in line 1 of \cref{alg:blocky_block}.  Considering that $P_{J_i} u^{(J_i)} = u^{(J_i)}$, we have \[u = \colmatfl{P_{J_1}}{P_{J_N}}^{\dag} \colmatfl{u^{(J_1)}}{u^{(J_N)}} =  \diag(\mu)^{-1} \colmatfl{P_{J_1}}{P_{J_N}}^* \colmatfl{u^{(J_1)}}{u^{(J_N)}} = D_\mu^{-1}\left(\sum_{i = 1}^N u^{(J_i)}\right).\]

\begin{algorithm}[htbp]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Blockwise Magnitude Estimation}
\label{alg:blocky_block}
\begin{algorithmic}[1]
    \REQUIRE $X \in T_{\delta, s}(\H^d)$, typically assumed to be an approximation $X \approx T_{\delta, s}(\ux \ux^*)$.  A $(T_{\delta, s}, d)$-covering $\{J_i\}_{i \in [N]}$.
    \ENSURE An estimate $\abs{x}$ of $\abs{\ux}$.
    \STATE For $j \in [d]$, set $\mu_j = \abs{\{i : j \in J_i\}}$ to be the number of appearences the index $j$ makes in $\{J_i\}$.
    \STATE For $i \in [N]$, set $u^{(J_i)}$ to be the leading eigenvector of $\abs{X^{(J_i)}} = \diag(\one_{J_i}) \abs{X} \diag(\one_{J_i})$, normalized such that $\norm{u^{(J_i)}}_2 = \sqrt{\norm{X^{(J_i)}}_2}$.
    \STATE Return $\abs{x} = D_\mu^{-1}\left(\sum_{i = 1}^N u^{(J_i)}\right)$.
    \end{algorithmic}
\end{algorithm}

We will denote the output of \cref{alg:blocky_block} by $\abs{x} = \BlkMag(X, \{J_i\})$.  In an overloading of notation, when the covering consists of intervals of length $m$, in the sense that $J_i = [m]_i$ for $i = 1, \ldots, d$, we will also write it as $\BlkMag(X, \{[m]_i\}_{i \in [d]}) = \BlkMag(X, m)$.  To include the case of ptychography, where these intervals are shifted by more than 1, we write \begin{equation}\begin{gathered} \BlkMag(X, m) = \BlkMag(X, \{[m]_i\}_{i \in [d]}), \ \text{and} \\ \BlkMag(X, (m, s)) = \BlkMag(X, \{[m]_{1 + s (\ell-1)}\}_{\ell \in [\dbar]}), \end{gathered}\label{eq:bm_overload} \end{equation} where we require $\dbar = \frac{d}{s}$ to be an integer.  In this way, the magnitude estimation technique used in \cref{sec:NumEval} is simply $\abs{x} = \BlkMag(X, \delta)$, whereas the $\abs{x_i} = \sqrt{X_{ii}}$ technique stated in \cref{alg:phaseRetrieval1} is equivalent to $\abs{x} = \BlkMag(X, 1)$.

Using \cref{cor:rank1Bound}, we are able to quickly prove a bound on the error of the estimate produced by this method.

\begin{proposition} \label{prop:blocky_block}
  Let $\{J_i\}_{i \in [N]}$ be a $(T_{\delta, s}, d)$-covering, and suppose $\uX = T_{\delta, s}(\ux \ux^*)$ for some $\ux \in \Cd$.  Using the notation of \cref{alg:blocky_block} (in particular, $\mu_j$ is as in line 1, and $\uu^{(J_i)} = \abs{\diag(\one_{J_i}) \ux}$), given $X \in T_{\delta, s}(\H^d)$, we have that the output $\abs{x}$ satisfies
  \begin{equation}
  \begin{aligned}
    \BlkMag(\uX, \{J_i\}) &= \abs{\ux} \\
    \norm{\BlkMag(X, \{J_i\}) - \abs{\ux}}_2 &\le \dfrac{\max_j \mu_j}{\min_j \mu_j} \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{(J_i)}}_2} \norm{\uX - X}_F.
  \end{aligned}
  \label{eq:blocky_rec}
  \end{equation}
  As special cases for $T_\delta(\Cdxd)$, we have
  \begin{equation}
    \begin{aligned}
      \norm{\BlkMag(X, m) - \ux}_2 &\le \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{[m]_i}}_2} \norm{X - \uX}_F \\
      \norm{\BlkMag(X, \delta) - \ux}_2 &\le \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{[\delta]_i}}_2} \norm{X - \uX}_F \\
      \norm{\BlkMag(X, 1) - \ux}_2 &\le \dfrac{\norm{\diag(X - \uX)}_F}{\min_i \abs{\ux_i}} 
    \end{aligned}
    \label{eq:blocky_spec}
  \end{equation}
\end{proposition}

\begin{proof}[Proof of \cref{prop:blocky_block}]
  The first inequality of \eqref{eq:blocky_rec} is clear, since line 2 of \cref{alg:blocky_block} will always return $\uu^{(J_i)} = \one_{J_i} \circ \abs{\ux}$, so line 3 will give \[ \left(D_\mu^{-1}\left(\sum_{i = 1}^N \uu^{(J_i)}\right)\right)_j = \frac{1}{\mu_j} \sum_{i = 1}^N \abs{\ux}_j \one_{j \in J_i} = \abs{\ux}_j.\]  %The second comes quite easily by applying \cref{cor:rank1Bound}\footnote{Here, we use the substitution $\eta \norm{\x_0}_2 = \frac{\norm{X - X_0}_F}{\norm{\x_0}_2}$.} and writing
  The second comes by writing \begin{equation} \norm*{D_\mu^{-1}(\sum_{i = 1}^N u^{(J_i)} - \uu^{(J_i)})}_2^2 \le \left(\frac{1}{\min_j \mu_j}\right)^2 \norm*{\sum_{i = 1}^N (u^{(J_i)} - \uu^{(J_i)})}_2^2. \label{eq:bb1} \end{equation}  From there, we consider that the $j\th$ term in the summation \begin{equation}\norm*{\sum_{i = 1}^N (u^{(J_i)} - \uu^{(J_i)})}_2^2 = \sum_{j = 1}^d \left(\sum_{i = 1}^N u^{(J_i)} - \uu^{(J_i)}\right)_j^2\label{eq:bb2}\end{equation} has at most $\max_k \mu_k$ nonzero summands, so, by $(\sum_{i=1}^n a_i)^2 \le n \sum_{i=1}^n a_i^2$, we have \begin{equation}\norm*{\sum_{i = 1}^N (u^{(J_i)} - \uu^{(J_i)})}_2^2 \le \max_j \mu_j \sum_{i = 1}^N (u^{(J_i)} - \uu^{(J_i)})^2.\label{eq:bb3}\end{equation}  We then apply \cref{cor:rank1Bound}\footnote{Here, we use the substitution $\eta \norm{\x_0}_2 = \frac{\norm{X - X_0}_F}{\norm{\x_0}_2}$.} to get \begin{equation} \sum_{i = 1}^N (u^{(J_i)} - \uu^{(J_i)})^2 \le (1 + 2 \sqrt{2}) \dfrac{\norm{\uu^{(J_i)} \uu^{(J_i)*} - X^{(J_i)}}_F^2}{\norm{\uu^{(J_i)}}_2^2} \label{eq:bb4} \end{equation}  In this expression, we consider that, in the summation $\sum_{i = 1}^N \norm{\uu^{(J_i)} \uu^{(J_i)*} - X^{(J_i)}}_F^2,$ the term $(\uX_{ij} - X_{ij})^2$ appears at most $\max\{\mu_i, \mu_j\}$ times, such that $\sum_{i = 1}^N \norm{\uu^{(J_i)} \uu^{(J_i)*} - X^{(J_i)}}_F^2 \le \max_j \mu_j \norm{\uX - X}_F^2$.  Using this substitution, combining \eqref{eq:bb1}--\eqref{eq:bb4}, and taking the square root of both sides gives \[\norm*{D_\mu^{-1}(\sum_{i = 1}^N u^{(J_i)} - \uu^{(J_i)})}_2 \le \dfrac{\max_j \mu_j}{\min \mu_j} \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{(J_i)}}_2} \norm{\uX - X}_F\] as desired.

  The first two inequalities of \eqref{eq:blocky_spec} are immediate by observing that $\mu_j = m$ when the covering is $\{[m]_i\}_{i \in [d]}$.  The third comes from setting $\epsilon_i = X_{ii} - \uX_{ii}$ and writing
  \begin{align*}
    \norm{\abs{x} - \abs{\ux}}_2^2 &= \sum_{i = 1}^d \left(\sqrt{X_{ii}} - \abs{\ux}_i\right)^2 = \sum_{i = 1}^d \left(\sqrt{\abs{\ux}_i^2 + \epsilon_i} - \sqrt{\abs{\ux}_i^2}\right)^2 \\
    &= \sum_{i = 1}^d \left(\frac{((\abs{\ux}_i^2 + \epsilon_i) - \abs{\ux}_i^2)^2}{\sqrt{\abs{\ux}_i^2 + \epsilon_i} + \abs{\ux}_i}\right)^2 \le \sum_{i = 1}^d \frac{\epsilon_i^2}{\min_i{\abs{\ux}_i^2}} \\
    &= \frac{\norm{\diag(X - \uX)}_F^2}{\min_i \abs{\ux}_i^2}
  \end{align*}  
\end{proof}

We end with a few remarks on the results of \cref{prop:blocky_block}.  One immediate benefit from \cref{eq:blocky_spec} is that the estimation error from $\BlkMag(X, 1)$ no longer scales poorly with $d^{1/4}$ as in \cref{lem:diag_mag_diff}.  The $\min_i \abs{\ux_i}$ factor in the denominator is also not a problem, since in the recovery results of, say, \cref{thm:MainRes} or \cref{cor:main_improve}, the same term (to a higher power) already appears in the phase-error expression.

In the error bound for $\BlkMag(X, m)$ in \cref{eq:blocky_spec}, we notice that the bound is \emph{strictly decreasing} with $m$, since, for $m_1 > m_2$, we have \[\min_i \norm{\ux^{[m_1]_i}}_2^2 \ge (m_1 - m_2) \min_i \abs{\ux_i}^2 + \min_i \norm{\ux^{[m_2]_i}}_2^2 \ge m_1 \min_i \abs{\ux_i}^2.\]  Also, considering \eqref{eq:blocky_rec}, it is clear that $\BlkMag(X, \delta)$ gives the absolute best bound over all $\{J_i\}$, since any $(T_\delta, d)$-covering $\{J_i\}_{i \in [N]}$ satisfies $J_i \subset [\delta]_\ell$ for some $\ell$.  This gives that $\min_i \norm{\uu^{(J_i)}}_2 \le \min_i \norm{\uu^{[\delta]_i}}_2$, and obviously $\frac{\max_j \mu_j}{\min_j \mu_j} \ge 1$, so the bound for $\BlkMag(X, \{J_i\})$ in \eqref{eq:blocky_rec} can never be better than that for $\BlkMag(X, \delta)$ in \eqref{eq:blocky_spec}.

We also remark that two easy ways to ensure $\frac{\max_j \mu_j}{\min_j \mu_j} = 1$ is minimized are to take some fixed $J_0 \subset [\delta]_0$ and let $J_\ell = J_0 + \ell$ be a ``cyclic'' covering, or to let $\{J_i\}$ be a partition of $[d]$.  These strategies will be relevant in \cref{sec:pty_std_rec}, but in the case of $s = 1$, $\BlkMag(X, \delta)$ always has the optimal bound for magnitude estimation error.

\subsection{Standard Recovery Algorithm for Ptychography}
\label{sec:pty_std_rec}
To recover $x$, an estimate of $\eit \ux$ from $X = \Ac^{-1}(y)$, we will not have to develop, nor even prove, any more technology.  The contents of \cref{ch:ang_sync,sec:con_number_ptych,sec:blocky_block} are sufficient to develop an algorithm, stated in \cref{alg:pty_pr} that is proven in \cref{thm:pty_rec} to stably produce an estimate of $\eit \ux$.  This algorithm differs very little from \cref{alg:pr_sdp}, except that in line 4, we use $\BlkMag(X, \{J_i\}) \circ \tbx$ instead of $\sqrt{\abs{\vec \diag(X)}} \circ \tbx$.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Phase Retrieval from Local Ptychographic Measurements}
\label{alg:pty_pr}
\begin{algorithmic}[1]
    \REQUIRE A family of masks $\{m_j\}_{j = 1}^D$ of support $\delta$; $s, d, \dbar \in \N$ satisfying $d = \dbar s \ge 2 \delta - 1$.  A $(T_{\delta, s}, d)$-covering $\{J_i\}_{i \in [N]}$.  Measurements $\y \approx \Ac(\ux \ux^*) \in \R^{\dbar D}$, as in \eqref{eq:pty_meas_op}.
    \ENSURE $x \in \Cd$ with $x \approx \eit \ux$ for some $\theta \in [0, 2 \pi]$.
    \STATE Compute the matrix $X = \Ac|_{T_{\delta, s}(\Cdxd)}^{-1} y \in T_{\delta, s}(\Hd)$ as an estimate of $T_{\delta, s}(\ux \ux^*)$.
    \STATE Form the banded matrix of phases, $\tX \in T_{\delta, s}(\Hd)$, by normalizing the non-zero entries of $X$. %(replacing any zero entries in the band with $1$'s)
    \STATE Compute $\hZ$, the solution to \eqref{eq:ang_sync_sdp} with $L = (2 \delta - 1) I - \tX$, and take $\tbx = \sgn(u),$ where $u$ is the top eigenvector of $\hZ$.
    \STATE Return $x = \BlkMag(X, \{J_i\}) \circ \tbx$.
\end{algorithmic}
\end{algorithm}

\Cref{thm:pty_rec} proves a bound on the accuracy of the output of \cref{alg:pty_pr}.  Here, we use the Laplacian matrix of the graph whose adjacency matrix is $T_{\delta, s}(\one \one^*) - I$, namely \begin{equation}D - W, \ \text{where} \ W = T_{\delta, s}(\one \one^*) - I \ \text{and} \ D = \diag(W \one).\label{eq:pty_lap}\end{equation}

\begin{theorem} \label{thm:pty_rec}
  Suppose we have a family of masks $\{m_j\}_{j \in [D]} \in \Cd$ of support $\delta$; $s, \dbar \in \N$ such that $s \dbar = d$; and a $(T_{\delta, s}, d)$-covering $\{J_i\}_{i \in [N]}$.  Further let $\ux \in \Cd, n \in \R^{\dbar D}$ be arbitrary and set $\uX = \ux \ux^*, X = \Ac^{-1}(\Ac(\uX) + n)$.  Define $\mu$ as in line 1 of \cref{alg:blocky_block}, $\tau_G = \lambda_2(D - W)$ for the matrices defined in \eqref{eq:pty_lap}, and $\SNR = \frac{\norm{\Ac(X)}_2}{\norm{n}_2}$%% , and $\alpha = \sum_{m = 1 - \delta}^{\delta - 1} \min\{s, \delta - \abs{m}\}$
  .  Then if line 3 of \cref{alg:pty_pr} solves \eqref{eq:ang_sync} exactly, then the output $x$ of \cref{alg:pty_pr} satisfies \begin{equation}\begin{aligned} \mintheta \norm{x - \eit \ux}_2 &\le \dfrac{4 \sqrt{2}}{\sqrt{\tau_G}} \dfrac{\sigma_{\min}^{-1} \norm{n}_2}{\abs{(\ux)_{\min}}^2} + \dfrac{\max_j \mu_j}{\min_j \mu_j} \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{(J_i)}}_2} \sigma_{\min}^{-1} \norm{n}_2 \\ \mintheta \norm{x - \eit \ux}_2 &\le \dfrac{4 \sqrt{2}}{\sqrt{\tau_G}} \dfrac{\kappa \norm{\uX}_F}{\abs{(\ux)_{\min}}^2 \SNR} + \dfrac{\max_j \mu_j}{\min_j \mu_j} \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{(J_i)}}_2} \kappa \frac{\norm{\uX}_F}{\SNR} \end{aligned} \label{eq:pty_rec}\end{equation}
\end{theorem}

\begin{proof}[Proof of \cref{thm:pty_rec}]
  This proof is a straightforward synthesis of the results in \cref{thm:improved_spec_pert,prop:blocky_block}.  In detail, we set $\tbx = \sgn(x), \tux = \sgn(\ux)$ and apply these two results by expanding
  \begin{align*}
    \mintheta \norm{x - \eit \ux}_2 &\le \mintheta \norm{\tbx - \eit \tux}_2 + \norm{\abs{x} - \abs{\ux}} \\
    &\le \dfrac{2 \sqrt{2} \norm{\tX - \widetilde{\uX}}_F}{\sqrt{\tau_G}} + \dfrac{\max_j \mu_j}{\min_j \mu_j} \dfrac{1 + 2 \sqrt{2}}{\min_i \norm{\uu^{(J_i)}}_2} \norm{X - \uX}_F.
  \end{align*}
  At this point, we quote \cref{lem:2d_srho} to get $\norm{\tX - \widetilde{\uX}}_F \le 2 \norm{X - \uX}_F / \abs{(\ux_{\min})}^2$ as usual, and finish by bounding $\norm{X - \uX}_F$ above by $\sigma_{\min}^{-1} \norm{n}_2$ and $\kappa \frac{\norm{\uX}_F}{\SNR}$.  
\end{proof}

We remark that a few elements of the bounds in \eqref{eq:pty_rec} are a bit unclear, most notably $\frac{\max_j \mu_j}{\min_j \mu_j}$ and $\tau_G$.  Of these, $\frac{\max_j \mu_j}{\min_j \mu_j}$ is a design feature that is chosen when we select our $(T_{\delta, s}, d)$-covering for the magnitude estimation step.  As is mentioned in \cref{sec:blocky_block}, one strategy for keeping this term under control would be making $\{J_i\}_{i \in [N]}$ a partition of $[d]$.  However, using the somewhat intuitive (if we follow the motivation behind \cref{sec:blocky_block}), maximal (it uses the largest possible $J_i$, which is an advantage for the $\min_i \norm{u^{(J_i)}}_2$ term) covering $J_i = \clopen{1 + (i - 1)s, \delta + 1 + (i - 1)s}$ for $i \in [\dbar]$, we notice that $\frac{\max_j \mu_j}{\min_j \mu_j} \in [1, 2]$.\footnote{The proof of this is brief: $j \in \clopen{1 + (\ell - 1) s, \delta + 1 + (\ell - 1)s}$ iff $\ell - 1 \in \clopen{\ceil{\frac{j- \delta}{s}}, \floor{\frac{j - 1}{s}} + 1}$, so that $\mu_j = \floor{\frac{j - 1}{s}} - \ceil{\frac{j - \delta}{s}} + 1$.  Clearly $\mu_j$ is periodic in $[s]$, so we take extrema over this interval.  Using $\floor{\frac{j-1}{s}} = 0$ for $j \in [s]$ and $-\ceil{\frac{j - \delta}{s}} = \floor{\frac{\delta - j}{s}}$, this immediately gives $\floor{\frac{\delta}{s}} \le \mu_j \le \floor{\frac{\delta - 1}{s}} + 1 \le \floor{\frac{\delta}{s}} + 1$.}

However, $\tau_G$ is still mysterious -- we have yet to obtain a convenient expression for $\tau_G$, so we relegate its study to the numerical experiments of \cref{sec:ptych_num}.

\subsection{Blockwise Vector Synchronization Method}
\label{sec:vecky_vec}
There is another method for retrieval from $X = \Ac^{-1}(y)$, based largely on the ideas behind the Blockwise Magnitude Estimation of \cref{sec:blocky_block}, that permits a very straightforward and satisfyingly general recovery method.  The idea here is to use something analogous to \cref{alg:blocky_block}, but which recovers the magnitudes \emph{and phases} of each block, and then stitches them together with the vector synchronization technique discussed in \cref{app:vec_sync}.  The blockwise, eigenvector-based step, called Blockwise Vector Estimation, is stated in \cref{alg:vecky_vec}, and the complete recovery process is stated in \cref{alg:pty_vec}.

Unfortunately, the theory behind this strategy is so far fairly weak: we are able to prove a recovery bound on it, but it suffers from tremendously poor scaling with problem dimension.  The proof is also very technical, so we defer it to \cref{app:vecky_vec}.  For the moment, we pose \cref{alg:pty_vec} as a theoretically unvetted, but intuitive and empirically promising technique, and defer a more optimistic view of its performance to the numerical studies of \cref{sec:ptych_num}.

\begin{algorithm}[htbp]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Blockwise Vector Estimation}
\label{alg:vecky_vec}
\begin{algorithmic}[1]
    \REQUIRE $X \in T_{\delta, s}(\H^d)$, typically assumed to be an approximation $X \approx T_{\delta, s}(\ux \ux^*)$.  A connected $(T_{\delta, s}, d)$-covering $(\{J_i\}_{i \in [N]}, G)$.
    \ENSURE Estimates $x^{(J_i)}$ of $\diag(\one_{J_i}) \ux$.
    \STATE For $i \in [N]$, set $x^{(J_i)}$ to be the leading eigenvector of $X^{(J_i)} = \diag(\one_{J_i}) X \diag(\one_{J_i})$, normalized such that $\norm{x^{(J_i)}}_2 = \sqrt{\norm{X^{(J_i)}}_2}$.
    \STATE Return $\BlkVec(X, (\{J_i\}, G)) = \rowmatfl{x^{(J_1)}}{x^{(J_N)}}$.
\end{algorithmic}
\end{algorithm}

Despite the pessimistic result of \cref{prop:vecky_vec}, we expect this method to perform fairly well.  Recalling the discussion of \cref{sec:weighted_graph} concerning the advantages of using the magnitudes of $X$ in the angular synchronization process (namely, that this will prioritize accurate relative phases between larger entries of $x$), it seems like a potentially sensible step forward to let the blockwise magnitude estimation process of \cref{alg:blocky_block} simultaneously produce an estimate of the phases of $x$.  This process operates by finding the eigenvectors of dense, approximately rank-1 matrices, and the eigenvector perturbation bound used in the proof of \cref{prop:blocky_block} works just as well for complex matrices.  Synthesizing these blockwise estimates by calculating appropriate relative phases between them (e.g., by appealing to the vector synchronization process defined in \cref{app:vec_sync}) and averaging as in line 3 of \cref{alg:blocky_block} seems satisfyingly symmetric, but our lack of theoretical understanding of the performance of the vector synchronization process prevents us from achieving a stronger theoretical promise on this method.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Phase Retrieval by Vector Synchronization}
\label{alg:pty_vec}
\begin{algorithmic}[1]
  \REQUIRE Measurements $y \approx \Ac(\ux \ux^*) \in \R^{\dbar D}$, where $\Ac$ is invertible on $T_{\delta, s}(\Cdxd)$ as in \eqref{eq:pty_meas_op}.  A connected $(T_{\delta, s}, d)$-covering $(\{J_i\}_{i \in [N]}, G')$.
  \ENSURE $x \in \Cd$ with $x \approx \eit \ux$ for some $\theta \in [0, 2 \pi]$.
  \STATE For $j \in [d]$, set $\mu_j = \abs{\{i : j \in J_i\}}$ to be the number of appearences the index $j$ makes in $\{J_i\}$.
  \STATE Set $V = \BlkVec(X, (\{J_i\}, G))$.
  \STATE Compute $\hZ$, the solution to \eqref{eq:ang_sync_sdp} with $L_{\rm vs}$ as defined in \eqref{eq:lap_vecky} using $V$ and $G'$ and set $\hz = \sgn(u),$ where $u$ is the top eigenvector of $\hZ$.
  \STATE Return $x = D_\mu^{-1} V \hz$.
\end{algorithmic}
\end{algorithm}

One remark that must be made before stating the vector synchronization-based recovery algorithm is that having a simple $(T_{\delta, s}, d)$-covering will not suffice for this method; we require a slightly stronger set of conditions on the sets $J_i \subset [d]$, which will allow vector synchronization to be performed on their blockwise estimates of $\ux$.  Namely, we define a \emph{connected $(T_{\delta, s}, d)$-covering} to be an ordered pair $(\{J_i\}, G')$ satisfying that $\{J_i\}$ is a $(T_{\delta, s}, d)$-covering and $G' = (V = [N], E)$ is a connected graph with \[E \subset \{(i, j) : J_i \cap J_j \neq \emptyset\},\] %% and weight matrix $W \in \R_+^{N \times N} \cap \Sc^n$ with $\supp(W) \subset E$,
which will permit the blocks $x^{(J_i)}$ associated with each $J_i$ to have a meaningful sense of relative phase over a connected graph.  The synthesis process will be to take, for each $i \in [N]$, $x^{(J_i)}$ to be the top eigenvector of $X^{(J_i)} = \diag(\one_{J_i}) X \diag(\one_{J_i})^*$ normalized such that $\norm{x^{(J_i)}}_2 = \sqrt{\norm{X^{(J_i)}}_2}$ and to synchronize them by taking the solution $\hz$ of \eqref{eq:ang_sync} for \begin{equation} L_{\rm vs} = D_{\rm vs} - W_{\rm vs} \circ X_{\rm vs}, \label{eq:lap_vecky} \end{equation} defining $D_{\rm vs}, W_{\rm vs}, X_{\rm vs}$ as in \eqref{eq:vec_sync_mats}.  With $\mu_j$ defined as in line 1 of \cref{alg:blocky_block}, the final estimate of $\ux$ is rendered as $x = D_\mu^{-1} \rowmatfl{x^{(J_1)}}{x^{(J_n)}} \hz$, which simply averages together the phase-synced $x^{(J_i)} \hz_i$ terms.  With this, we state \cref{alg:vecky_vec,alg:pty_vec}.\footnote{In \cref{alg:pty_vec}, notice that $\BlkVec(X, (\{J_i\}, G))$ is the output of \cref{alg:vecky_vec}.}

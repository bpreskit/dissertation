In this section, we use the results of \cref{sec:con_number} to explicitly state the inverse of the measurement operator $\Ac$, as well as the computational complexity of calculating its inverse.  Additionally, we calculate the variance in \emph{each entry} of $\Ac^{-1}(y)$ when $y = \Ac(T_\delta(x x^*)) + \eta$ is produced under an i.i.d.~Gaussian noise model, which will be useful to us in later analysis.

\subsection{Explicit inverse of $\Ac$}
\label{sec:inv_runtime}

We begin by fixing a local measurement system $\{m_j\}_{j = 1}^D$ with support $\delta$, taking $\Ac$ to be the associated measurement operator and $A$ its canonical matrix representation as in \cref{eq:meas_op,eq:meas_mat}.  We then remark from \eqref{eq:interleaved_meas} and \cref{lem:circ_diag} that \[A = P^{(D, d)} (F_d \otimes I_D) \diag(M_\ell)_{\ell = 1}^d (F_d \otimes I_D)^* P^{(d, D)},\]  where we recall $M_\ell$ from \eqref{eq:M_ell}.  In the case where $\{m_j\}_{j = 1}^D$ is a local Fourier measurement system with mask $\gamma$ and modulation index $K$, we define $Z \in \C^{D \times d}$ by \begin{equation} Z_{m \ell} = \sqrt{D} f_\ell^{d*} g_{m - \delta}. \label{eq:four_shift_mat}\end{equation}  Setting $z_\ell = Z e_\ell$, we have \begin{equation} M_\ell = \tF_D D_{z_\ell} \ \text{and} \ \diag(M_\ell)_{\ell = 1}^d = (I_d \otimes \tF_D) \diag(\vec(Z)), \label{eq:M_rearr} \end{equation} by which we further reduce $A$ to \[A = P^{(D, d)} (F_d \otimes I_D) (I_d \otimes \tF_D) \diag(\vec(Z)) (F_d \otimes I_D)^* P^{(d, D)}.\]  This reasoning immediately produces the inverse of $A$, which we state in \cref{prop:A_inv_gam}.  While \cref{prop:A_inv_gam} covers the case of general local measurement systems, the rest of this section will be restricted to local Fourier measurement systems.

\begin{proposition}
  Let $A \in \C^{d \times 2 \delta - 1}$ be the canonical representation of the measurement operator $\Ac$ associated with a local Fourier measurement system $\{m_j\}_{j = 1}^d$ of support $\delta \le \frac{d + 1}{2}$ with mask $\gamma \in \R^d$.  Defining $Z$ as in \eqref{eq:four_shift_mat}, we have
  \begin{equation} A^{-1} = P^{(D, d)} (F_d \otimes I_D) (\diag(\vec(Z)))^{-1} (I_d \otimes \tF_D^*) (F_d \otimes I_D)^* P^{(d, D)}. \label{eq:A_inv_gam} \end{equation}

  If $\{m_j\}_{j = 1}^D$ is a general local measurement system, and $\Ac$ is invertible, then its inverse is given by \[A = P^{(D, d)} (F_d \otimes I_D) \diag(M_\ell^{-1})_{\ell = 1}^d (F_d \otimes I_D)^* P^{(d, D)}.\]
  \label{prop:A_inv_gam}
\end{proposition}

This formulation makes it straightforward to deduce the computational complexity of inverting $\Ac$, as previously stated in \cref{sec:RuntimeAlg1}.  Namely, each permutation $P^{(D, d)}$ may be run on a vector with $\bigO(d D)$ operations.  Since, by \eqref{eq:interkron_swap} of \cref{lem:interkron}, we have that $F_d \kron I_D = P^{(d, D)}(I_D \kron F_d) P^{(D, d)}$, and considering also that $I_D \kron F_d$ comprises $D$ Fourier transforms of dimension $d$, multiplication by $F_d \kron I_D$ costs $\bigO(d D + D d \log d) = \bigO(d D \log d)$ operations.  Since $\tF_D = F_D S^{1-\delta}$, multiplying by $I_d \kron \tF_D^*$ takes $\bigO(d D \log D)$ operations.  Finally, multiplying by $\diag(\vec(Z))^{-1}$ trivially has a cost of $\bigO(d D)$.  Putting all these considerations together, and recalling that $d \ge D = 2 \delta - 1$, the cost of inverting $A$ comes out to \[\bigO(d D) + \bigO(d D \log d) + \bigO(d D \log D) + \bigO(d D) + \bigO(d D \log d) + \bigO(d D) = \bigO(d D \log d),\] or $\bigO(\delta\, d \log d)$, as concluded in \cref{sec:RuntimeAlg1} and \cite{IVW2015_FastPhase}.

\subsection{Preliminaries in Probability}
Prior to discussing the variance of the images of vectors under $A^{-1}$ or $\Ac^{-1}$, we review some preliminaries regarding the real and complex multivariate Gaussian distributions.  These results and the notation with which we express them may be found in many standard texts, for example \cite{tong1990normal} for the real case and section 7.8.1 of \cite{gallager2008digital} for the complex.  For $\mu \in \Rn$ and $0 \prec \Sigma \in \sym^n, \Nc(\mu, \Sigma)$ refers to the multivariate normal distribution on $\Rn$ with mean $\mu$ and covariance matrix $\Sigma = \mathbb{E}_x[(x - \mu) (x - \mu)^T]$, and is determined by its probability density function \[f_{\Nc}(x; \mu, \Sigma) = \frac{1}{(2 \pi)^{n / 2} \det(\Sigma)^{1 / 2}} \exp\left(- \dfrac{(x - \mu)^T \Sigma^{-1} (x - \mu)}{2}\right).\]  For $\nu \in \Cn$ and $0 \prec \Xi \in \H^n, \CN(\nu, \Xi)$ is the (circularly symmetric about $\nu$) multivariate complex normal distribution on $\Cn$ with mean $\nu$, covariance $\Xi = \mathbb{E}_z[(z - \nu) (z - \nu)^*]$, and density function \[f_{\CN}(z; \nu, \Xi) = \dfrac{1}{\pi^n \det(\Xi)} \exp(- (z - \nu)^* \Xi^{-1} (z - \nu))\]  We remark that circular symmetry is defined by having that the real and imaginary parts of $z \sim \CN(0, \Xi)$ be i.i.d., and is ensured by tacitly requiring, as we shall throughout this dissertation, that $\mathbb{E}[(z - \nu) (z - \nu)^T] = 0$ (see Theorem 7.8.1 of \cite{gallager2008digital}).

We now relate a standard result from the literature concerning linear transformations of Gaussian random vectors.
\begin{proposition}[Theorem 3.3.3 in \cite{tong1990normal} and Section 7.8.1 of \cite{gallager2008digital}]  
  Suppose $x \sim \Nc(\mu, \Sigma)$.  Then for $A \in \Rmxn, b \in \Rm,$ \begin{equation} Ax + b \sim \Nc(A \mu + b, A \Sigma A^T). \label{eq:real_gausaff} \end{equation}

  Suppose $z \sim \CN(\nu, \Xi)$.  Then for $B \in \Cmxn, c \in \Cm,$ \begin{equation} B z + c \sim \CN(B \nu + c, B \Xi B^*). \label{eq:cpx_gausaff}\end{equation}
  \label{prop:gausaff}
\end{proposition}
We remark that the result regarding complex Gaussian vectors implies that linear transformations preserve the property of circular symmetry.

\subsection{Distribution of variance}
\label{sec:dist_var}
In the interest of studying the propagation of error through our recovery algorithm, in this section, we describe the probability distribution of the noise on each entry of $\Ac^{-1}(\Ac(x x^*) + \eta)$ as a function of the noise vector's distribution.  To keep things tractable, we assume that the entries of $\eta$ are identically and independently distributed; specifically, we will assume that $\eta_{(\ell, j)} \iid \Nc(0, \sigma^2)$ for some $\sigma \ge 0$.

Before we begin, we remark that the distribution of $\Ac^{-1}(\eta)_{i, i + m}$ will depend only on $m$.  This follows intuitively by noting that $\Ac$ commutes nicely with ``diagonal shifts'' in the sense that \[\Ac(S^k X S^{-k})_{(\ell, j)} = \inner{S^{\ell} m_j m_j^* S^{-\ell}, S^k X S^{-k}} = \inner{S^{\ell - k} m_j m_j^* S^{-(\ell - k)}, X} = \Ac(X)_{(\ell - k, j)},\] so that if $y_1, y_2 \in \R^{[d] \times [D]}$ satisfy $(y_1)_{\ell, j} = (y_2)_{\ell - k, j}$ for some $k \in \N$, we will have $\Ac^{-1}(y_1) = S^k \Ac^{-1}(y_2) S^{-k}$.  In particular, if the entries of $y_1$ are identically, independently distributed random variables and $y_2$ is \emph{defined} by $(y_2)_{\ell - k, j} = (y_1)_{\ell, j}$, then $y_1$ and $y_2$ are drawn from the same distribution on $\R^{[d] \times [D]}$.  This means that $\Ac^{-1}(y_1)$ and $\Ac^{-1}(y_2)$ are identically distributed, but $\Ac^{-1}(y_1) = S^k \Ac^{-1}(y_2) S^{-k}$, so the distributions of $\Ac^{-1}(y_1)$ and $S^k \Ac^{-1}(y_1) S^{-k}$ are identical.  In particular, the distribution of the image of i.i.d.~noise under $\Ac^{-1}$ is invariant under such diagonal shifts, so $\Ac^{-1}(\eta)_{i, i + m}$ is distributed identically to (though not necessarily independently from!) $\Ac^{-1}(\eta)_{1, 1 + m}$, and this conclusion holds for all $i$ and $m$.

To make this precise, and to discover the distribution of $\Ac^{-1}(\eta)_{1, 1 + m}$ exactly, we will present another means by which $\Ac^{-1}(y)$ may be calculated from $y$.  With this in mind, we remark that \eqref{eq:interkron_mat} of \cref{lem:interkron} gives that \[P^{(D, d)} (F_d \kron I_D) = \rowmat{I_D \kron f^d}{1}{d},\] so $A$ may be transformed by \eqref{eq:kron_diag} to give \[A = \rowmatfun{M_@ \kron f_@^d}{1}{d} \colmat{I_D \kron f^d}{1}{d} = \sum_{j = 1}^d M_j \kron f_j^d f_j^{d*}.\]  Setting $X = \rowmat{\chi}{1 - \delta}{\delta - 1} \in \C^{d \times 2 \delta - 1}$, \cref{lem:kronvec} gives us \[A \colmat{\chi}{1 - \delta}{\delta - 1} = \vec\left(\sum_{j = 1}^d f_j^d f_j^{d*} X M_j^T\right).\]  Recalling \eqref{eq:M_rearr} along with $\tF_D^{-T} = (\tF_D^T)^* = \conj{\tF}_D$, we have \[f_j^{d*} \mat_{(d, D)}\left(A \colmat{\chi}{1 - \delta}{\delta - 1}\right) \conj{\tF}_D = f_j^{d*} X D_{z_j},\]
 so that, for $\ell \in [2 \delta - 1]$, and recalling $\conj{\tF}_D e_\ell = \conj{f}^D_{\ell + 1 - \delta} = f_{\delta + 1 - \ell}^D$, we have \[f_j^{d *} \mat_{(d, D)}(A \vec(X)) f_{\delta + 1 - \ell}^D = f_j^{d*} \mat_{(d, D)}(A \vec(X)) \conj{\tF}_D e_\ell = f_j^{d*} \chi_{\ell - \delta} Z_{\ell j}.\] In this way, from $A \vec(X)$ we may recover \[b_{\ell} := F_d^* \mat_{(d, D)}(A \vec(X)) f_{\delta + 1 - \ell}^D = \vec(f_j^{d*} Z_{\ell j} \chi_{\ell - \delta})_{j = 1}^d = D_{Z^T e_{\ell}} F_d^* \chi_{\ell - \delta}\] for each $\ell$, from which $\chi_{\ell - \delta}$ is determined by taking $\chi_{\ell - \delta} = F_d D_{Z^T e_{\ell}}^{-1} b_\ell$.
In other words, for $y \in \R^{d D}$, the $m\th$ diagonal of $\Dc_\delta^{-1}(A^{-1} y)$, for $m = 1 - \delta, \ldots, \delta - 1$, is given by \begin{equation} \chi_m = F_d D^{-1}_{Z^T e_{m + \delta}} F_d^* \mat_{(d, D)}(y) f_{1 - m}^D, \label{eq:chi_m} \end{equation} and we use this expression to deduce the distribution of noise on the $m\th$ diagonal when $y$ is a random variable.  For instance, if we consider that \[\Dc_\delta^{-1} A^{-1}(A \Dc_\delta(T_\delta(x x^*)) + \eta) = T_\delta(x x^*) + \Dc_\delta^{-1} (A^{-1} \eta),\] where $\eta_j \iid \Nc(0, \sigma^2)$ for $j \in [d (2 \delta - 1)]$, knowing the distribution of $A^{-1} \eta$ will tell us the distribution of noise on our recovered estimate of $T_\delta(x x^*)$.  We deduce this result directly from \eqref{eq:chi_m}, and summarize it in \cref{prop:var_dist}.  We remark that, in the statement and proof of this proposition, exponentiation of vectors is entry-wise.
\begin{proposition}
  Suppose that $\{m_j\}_{j = 1}^{2 \delta - 1}$ is a local Fourier measurement system with support $\delta \le \frac{d + 1}{2}$, mask $\gamma$, and modulation index $K = D = 2 \delta - 1$, with associated measurement operator and representation matrix $\Ac$ and $A$.  Suppose further that $\eta \in \R^{d (2 \delta - 1)}$ has entries that are i.i.d.~Gaussian random variables, namely $\eta_j \iid \Nc(0, \sigma^2)$ for $j \in [d (2 \delta - 1)]$ and some $\sigma \ge 0$.  Then, setting $N = \Dc_\delta^{-1} A^{-1} \eta \in T_\delta(\Cdxd)$, then $N$ is Hermitian, its $0, \ldots, \delta - 1\st$ diagonals are distributed  independently from one another, and the $m\th$ diagonal of $N$ is distributed by \begin{gather*} \diag(N, m) \sim \CN(0, \sigma^2 \circop(s_m)), m \in [\delta - 1], \ \text{and} \\ \diag(N, 0) \sim \Nc(0, \sigma^2 \circop(s_0)),\end{gather*} where $s_m = \frac{1}{D \sqrt{d}} F_d^* (F_d^* g_m)^{-2}$.%  The diagonals $m = 0, \ldots, \delta - 1$ are distributed, while the diagonals $m = 1 - \delta, \ldots, -1$ are determined by Hermitianity of $N$, namely $\diag(N, m) = \conj{\diag(N, -m)}$.
  \label{prop:var_dist}
\end{proposition}

\begin{proof}[Proof of \cref{prop:var_dist}]
  To establish that $N$ is Hermitian, we remark that $\Cdxd = \H^d \oplus \Skew(d)$, where \[\Skew(d) = \{B \in \Cdxd : B^* = -B\}\] is the set of skew-Hermitian matrices and where $\oplus$ represents the direct product.  In other words, given any $M \in \Cdxd$, we have a unique $H \in \H^d, B \in \Skew(d)$ such that $M = H + B$.  We now consider that, if $H \in \H^d$ and $B \in \Skew(d)$, we have that
  \begin{align*}
    \Tr(H^* B) &= \Tr(H B) = \Tr(B H) \\
    &= -\Tr(B^* H) = - \conj{\Tr(H^* B)},
  \end{align*}
  meaning that $\Tr(H^* B) \in \ii \R$.  Additionally, we remark that the Hermitian matrices are a real Hilbert space, so for another $H' \in \H^d$, we have $\Re \inner{H', M} = \inner{H', H}$ and $\Im \inner{H', M} = \inner{H', B} / \ii$.  Therefore, since all the measurement matrices $S^\ell m_j m_j^* S^{- \ell}$ appearing in $\Ac$ are Hermitian, given $M \in \Cdxd$ decomposed into its Hermitian and skew-Hermitian parts $M = H + B$, we have that $\Re \Ac(M) = \Ac(H)$ and $\Im \Ac(M) = \Ac(B) / \ii$.  In particular, $\Ac^{-1}(\R^{[d]_0 \times [D]}) \subset \H^d$, and $N$, being the inverse image of a real vector $\eta$, will be Hermitian.

  For convenience, throughout the remainder of the proof we will set $\chi_m = \diag(N, m)$ for $m = 1 - \delta, \ldots, \delta - 1$.  To prove the independence of $\{\chi_m\}_{m \in [\delta]_0}$, we consider that \eqref{eq:chi_m} tells us that \[\chi_m = F_d D^{-1}_{Z^T e_{m + \delta}} F_d^* \left(\mat_{(d, D)}(\eta) f^D_{1 - m}\right),\] and we focus on the term $\mat_{(d, D)}(\eta) f_{1 - m}^D$.  Considering that $\eta \sim \Nc(0, \sigma^2 I_{d D})$, we have that the rows $\rowmat{r}{1}{d}^T$ of $\mat_{(d, D)}(\eta)$ are distributed according to $r_i = \mat_{(d, D)}(\eta)^T e_i \sim \Nc(0, \sigma^2 I_D)$.  At this point, it would be convenient if we could merely cite \cref{prop:gausaff} to establish the distribution of $r_i^T f^D_{1 -m }$ or indeed $\mat_{(d, D)}(\eta) f_{1 - m}^D$, but we remark that we don't have a result for the image of a real Gaussian vector under a complex linear transformation.  Therefore, we consider the real and imaginary parts of $\mat_{(d, D)}(\eta) f_{1 - m}^D$ separately, setting $v_m = \mat_{(d, D)}(\eta) f_{1 - m}^D$ and seeing that \[(v_m)_i = r_i^T \Re(f_{1 - m}^D) + \ii\, r_i^T \Im(f_{1 - m}^D) = \Re(f_{1 - m}^D)^T r_i + \ii\, \Im(f_{1 - m}^D)^T r_i.\]  Since $\{f_1^D\} \cup \{\sqrt{2} \Re(f_{1 - m}^D)\}_{m \in [\delta - 1]} \cup \{\sqrt{2} \Im(f_{1 - m}^D)\}_{m \in [\delta - 1]}$ is an orthonormal basis for $\R^D$, then compiling these vectors into a matrix $Q$, we have from \cref{prop:gausaff} that $Q^T r_i \sim \Nc(0, \sigma^2 Q^T Q) = \Nc(0, \sigma^2 I_D)$, meaning the real and imaginary components of $v_1, \ldots, v_{\delta - 1}$ and $v_0$ are all independent, with $v_0 \sim \Nc(0, \sigma^2 I_d), \Re(v_m) \sim \Nc(0, \frac{\sigma^2}{2} I_d),$ and $\Im(v_m) \sim \Nc(0, \frac{\sigma^2}{2} I_d)$ for $m \in [\delta - 1]$.  Therefore, $v_m \iid \CN(0, \sigma^2 I_d)$ and $v_0 \sim \Nc(0, \sigma^2 I_d)$, independently from the other $v_m$.  Since the $v_m$ are independent, clearly their images under non-singular, fixed (independently of the random process) linear transformations will also be independent.  In particular, the diagonals $\chi_m = F_d D^{-1}_{Z^T e_{m + \delta}} F_d^* v_m$ will be independent of one another for $m \in [\delta]_0$.

  To get the actual distribution of the $\chi_m$ for $m \in [\delta - 1]$, we simply quote \cref{prop:gausaff} again to get that \[\chi_m \sim \CN(0, \sigma^2 F_d D^{-1}_{Z^T e_{m + \delta}} F_d^* I_d F_d D^{-1}_{Z^T e_{m + \delta}} F_d^*) = \CN(0, \sigma^2 F_d D^{-2}_{Z^T e_{m + \delta}} F_d^*).\] The covariance matrix becomes, by recalling \eqref{eq:circ_dft_diag} and the definition of $Z$ in \eqref{eq:four_shift_mat}, \[\sigma^2 F_d D^{-2}_{Z^T e_{m + \delta}} F_d^* = \circop\left(d^{-1/2} F_d^* (Z^T e_{m + \delta})^{-2}\right) = \circop(s_m).\]  The only distinction for $\chi_0$ is that the distributions are all $\Nc$ instead of $\CN$; the same calculations as above give $\chi_0 \sim \Nc(0, \sigma^2 \circop(s_0))$.  To verify that $s_0 \in \Rd$, we consider that $g_0 = \gamma \circ \gamma$, so $\tilde{g} := F_d^* g_0 = (F_d^* \gamma) * (F_d^* \gamma) $ satisfies $\tilde{g}_i = \tilde{g}_{2 - i}$ (equivalently, $R \tilde{g} = \tilde{g}$).  Similarly, $R(\tilde{g}^{-2}) = \tilde{g}^{-2}$, which guarantees that $s_0 = \frac{1}{D \sqrt{d}} F_d^* \tilde{g}^{-2} \in \R^d$.
\end{proof}
